{"cells":[{"cell_type":"markdown","metadata":{},"source":["Cui Yulong [190198653]\n","\n","I have modified my_utils.py file, adding a write() function to train_ch3() that creates a .txt file to record test accuracy. (See line 329 in my_utils.py)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649164360360,"user":{"displayName":"Tony Cui","userId":"04197346257785096591"},"user_tz":-60},"id":"zf8t0dcOEdN8"},"outputs":[],"source":["# Importing libraries\n","import my_utils as mu\n","import torch\n","from torch import nn"]},{"cell_type":"markdown","metadata":{},"source":["Task 1 Read dataset and create dataloaders\n","\n","The default batch_size was 256, but after a number of tests using different values of batch_size such as 200, 128, 64, I found that 250 gives me the best test accuracy overall. \n","\n","train_iter, test_iter are the dataloaders loading fashion_mnist using the method predefined in my_utils.py."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":280,"status":"ok","timestamp":1649164361708,"user":{"displayName":"Tony Cui","userId":"04197346257785096591"},"user_tz":-60},"id":"WSmDhqR4HRUl","outputId":"4c59d83a-891a-49ae-d00a-acebae7d9a4e"},"outputs":[],"source":["# Initialising batch size\n","batch_size = 250\n","# Data loader\n","train_iter, test_iter = mu.load_data_fashion_mnist(batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["Task 2 Stem and Backbone\n","\n","Stem\n","\n","I used functional.unfold() instead of unfold().unfold() or reshape() in previous attempts as it is a better function to provide patching. \n","Batch Normalisation is used to standardise inputs to linear layers which improves test accuracy.\n","\n","\n","Backbone\n","\n","I used 2 MLPs, 2 ReLU activation softmax regression functions and 3 batch normalisations for my backbone."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":363,"status":"ok","timestamp":1649164483818,"user":{"displayName":"Tony Cui","userId":"04197346257785096591"},"user_tz":-60},"id":"-Pj26dyxG3h3"},"outputs":[],"source":["# Net class\n","class Net(torch.nn.Module):\n","    def __init__(self, num_inputs, num_hidden, num_outputs):\n","      # Constructor\n","      super(Net, self).__init__()\n","      # Initialise variables\n","      self.num_inputs = num_inputs\n","      self.num_hidden = num_hidden\n","      self.num_outputs = num_outputs\n","\n","      # Stem Linear layer\n","      self.LinearStem = nn.Linear(49,49)\n","      # Stem linear layer normalisation\n","      torch.nn.init.normal_(self.LinearStem.weight, std=0.01)\n","      torch.nn.init.zeros_(self.LinearStem.bias)\n","      # Stem batch normalisation 1D\n","      self.batchNormStem = nn.BatchNorm1d(num_inputs)\n","\n","      # Backbone Linear layers \n","      # MLP 1\n","      self.LinearB1 = nn.Linear(num_inputs, num_hidden)\n","      # Backbone linear layer normalisation\n","      torch.nn.init.normal_(self.LinearB1.weight, std=0.01)\n","      torch.nn.init.zeros_(self.LinearB1.bias)\n","      # Backbone batch normalisation 1D\n","      self.batchNormBL1 = nn.BatchNorm1d(49)\n","      self.LinearB2 = nn.Linear(num_hidden,30)\n","      torch.nn.init.normal_(self.LinearB2.weight, std=0.01)\n","      torch.nn.init.zeros_(self.LinearB2.bias)\n","      self.batchNormBL2 = nn.BatchNorm1d(49)\n","\n","      # MLP 2\n","      self.LinearB3 = nn.Linear(49, num_hidden)\n","      torch.nn.init.normal_(self.LinearB3.weight, std=0.01)\n","      torch.nn.init.zeros_(self.LinearB3.bias)\n","      self.batchNormBL3 = nn.BatchNorm1d(30)\n","      # No need for batch normalisation for final layer\n","      # Output from final layer gets passed into Classifier\n","      self.LinearB4 = nn.Linear(num_hidden,10)\n","      torch.nn.init.normal_(self.LinearB4.weight, std=0.01)\n","      torch.nn.init.zeros_(self.LinearB4.bias)\n","\n","      # Activation function\n","      self.relu = nn.ReLU()\n","\n","\n","    def forward(self, image):\n","      # Stem\n","      # Unfold image\n","      imageUnfold = nn.functional.unfold(image, (4,4), stride=4)\n","      # Pass through linear layer and normalise batch\n","      out_stem = self.batchNormStem(self.LinearStem(imageUnfold))\n","\n","      # Backbone\n","      # Block (each block uses 2 MLPs)\n","      # Transpose stem output\n","      out_backbone = torch.transpose(out_stem,1,2)\n","      # Pass through linear layer and normalise batch\n","      out_backbone = self.batchNormBL1(self.LinearB1(out_backbone))\n","      # Activate softmax regression\n","      out_backbone = self.relu(out_backbone)\n","      out_backbone = self.batchNormBL2(self.LinearB2(out_backbone))\n","      \n","      out_backbone = torch.transpose(out_backbone,1,2)\n","      out_backbone = self.batchNormBL3(self.LinearB3(out_backbone))\n","      out_backbone = self.relu(out_backbone)\n","      out_backbone = self.LinearB4(out_backbone)\n","\n","      #  Pass to Classifier\n","      return out_backbone.mean(axis=1)\n","\n","# Define input, hidden and output\n","num_inputs, num_hidden, num_outputs = 16, 250, 10\n","\n","# Create Net class instance using defined values\n","net = Net(num_inputs, num_hidden, num_outputs)"]},{"cell_type":"markdown","metadata":{},"source":["Task 3 Loss % Optimizer\n","\n","I used the default loss function nn.CrossEntropyLoss() that was given to us in the labs.\n","\n","I changed the default optimizer SGD() to NAdam() as I have tried many test runs with SGD() and Adam() as well as NAdam(), NAdam() has given me the best test accuracy so far."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1649164365942,"user":{"displayName":"Tony Cui","userId":"04197346257785096591"},"user_tz":-60},"id":"mFp0mygNfsnF"},"outputs":[],"source":["# Loss\n","loss = nn.CrossEntropyLoss()\n","\n","# Learning rate, Weight decay\n","lr, wd = 0.0009, 0.00001\n","\n","# NAdam optimizer \n","optimizer = torch.optim.NAdam(net.parameters(), lr=lr, weight_decay=wd)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":562},"executionInfo":{"elapsed":444900,"status":"error","timestamp":1649164930335,"user":{"displayName":"Tony Cui","userId":"04197346257785096591"},"user_tz":-60},"id":"OCBN-jkHf0Iv","outputId":"5fc3963d-6317-4039-fc4c-1c0dba142e3c"},"outputs":[],"source":["# Define time epoch\n","num_epochs = 30 \n","# Execute using predefined train_ch3() in my_untils.py\n","mu.train_ch3(net, train_iter, test_iter, loss, num_epochs, optimizer)\n","# mu.train_ch6(net, train_iter, test_iter, num_epochs, optimizer)\n","# Testing Accuracy\n","mu.evaluate_accuracy(net, test_iter)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM8JWImCAZvMoMK7TRuXguZ","collapsed_sections":[],"name":"cw.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"}},"nbformat":4,"nbformat_minor":0}
